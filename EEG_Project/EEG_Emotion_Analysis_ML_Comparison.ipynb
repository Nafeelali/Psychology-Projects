{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":218459,"sourceType":"datasetVersion","datasetId":93959}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# EEG EMOTION CLASSIFICATIONS: Comparing Random Forest VS Logistic Regression","metadata":{"_uuid":"a7c79bc7-d7bf-4258-a5a6-52fe56bf74d6","_cell_guid":"d2d04983-fa62-416f-ab07-be1552a7c7f3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Introduction\n\nElectroencephalography (EEG) measures the electrical activity of the brain and can provide insights into different emotional states. This project is an opportunity to explore how EEG signals reflect emotions and, more importantly, to understand how different machine learning models handle this type of data.\n\nI aim to compare the performance of two commonly used algorithms, Random Forest and Logistic Regression, not just to achieve high accuracy, but to learn how model choice impacts predictions, interpretability, and feature importance.\n\nRandom Forest is a tree-based ensemble method capable of capturing complex, non-linear patterns, while Logistic Regression is a linear model that predicts class probabilities and is simpler to interpret.\n\nThe dataset contains EEG recordings labelled as POSITIVE, NEUTRAL, or NEGATIVE. By evaluating these models using accuracy, confusion matrices, F1-scores, and feature importance, I hope to gain a deeper understanding of how different modelling approaches work on the same data and what each can teach me about EEG-based emotion recognition.","metadata":{"_uuid":"82bf90b6-8996-4d97-ba21-a40340615e7d","_cell_guid":"f3a51f06-5d53-4b0a-a565-e91831ca8bcf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Methods\n### Dataset\nSource: Kaggle EEG Brainwave Dataset â€“ Feeling Emotions.\n\nFeatures: 14 EEG channel measurements per recording, plus additional extracted features (FFT and statistical metrics).\n\nTarget: label (POSITIVE, NEUTRAL, NEGATIVE).\n\nTotal samples: 2,132 (1,705 training, 427 testing).","metadata":{"_uuid":"c1363197-fe99-4add-bb9c-b0dbe72c4c57","_cell_guid":"8bca4d15-7c62-4554-8328-7672d6d9791a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"\n# 1. IMPORT LIBRARIES\n\n# Explanation:\n# pandas & numpy -> data handling,\n# sklearn -> modeling and evaluation,\n# matplotlib & seaborn -> visualization.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"_uuid":"96836e95-2cec-4a13-bc3d-21fb00c4682f","_cell_guid":"0d36ff66-b82b-403f-b760-75c784a6f9ff","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#2. Load Data \n\n# Explanation:\n# Load the EEG dataset\ndf = pd.read_csv('/kaggle/input/eeg-brainwave-dataset-feeling-emotions/emotions.csv')\n\n# Inspect the first few rows\ndf.head()","metadata":{"_uuid":"624209b2-86fa-474c-913f-0f6e69dcb7cb","_cell_guid":"7a437828-427c-451c-8092-6e5e62c9c715","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. PREPARE FEATURES AND LABELS\n\n# Explanation:\n# Features X -> all columns EXCEPT 'label'.\n# Labels y -> the 'label' column.\nX = df.drop('label', axis=1)\ny = df['label']\n\n# Check label distribution\nprint(\"Label Distribution:\\n\", y.value_counts())","metadata":{"_uuid":"d50bfb2e-0fc0-4eb0-90cb-2977960d8101","_cell_guid":"1ed4a486-714a-45c9-b32c-78a2900be636","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. SPLIT DATA INTO TRAIN AND TEST SETS\n\n# Explanation:\n# We split 80% training, 20% testing.\n# stratify=y ensures each emotion is represented proportionally.\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(\"Training samples:\", len(X_train))\nprint(\"Testing samples:\", len(X_test))\nprint(\"Training label distribution:\\n\", y_train.value_counts())\nprint(\"Testing label distribution:\\n\", y_test.value_counts())","metadata":{"_uuid":"47ac78a2-c80d-41ff-b0f4-55f24c3ea281","_cell_guid":"99e4580e-efa4-4de8-a949-c5f1aed2f6dd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. SCALE DATA FOR LOGISTIC REGRESSION\n# Explanation:\n# Logistic Regression works better when features are scaled.\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","metadata":{"_uuid":"9c8e67c2-4982-4c36-a100-5b2275af25bf","_cell_guid":"a0fb62cf-b8ba-46db-881f-d3b451bba495","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. LOGISTIC REGRESSION\n# Explanation:\n# Train logistic regression and evaluate.\nlog_reg = LogisticRegression(max_iter=1000, random_state=42)\nlog_reg.fit(X_train_scaled, y_train)\ny_pred_log = log_reg.predict(X_test_scaled)\n\nprint(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_log))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_log))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_log))","metadata":{"_uuid":"ae45d39f-af2f-40f8-b4b2-d35898b4427d","_cell_guid":"12773fda-0d2d-4c84-856f-4ebdcdbcfb99","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 7. RANDOM FOREST\n# Explanation:\n# Random Forest is an ensemble method that can handle unscaled features.\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\n\nprint(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))","metadata":{"_uuid":"1c1d0e7d-f944-471f-aba8-f196d42c3bf1","_cell_guid":"50439de8-bca7-4705-9a14-88a61363f68b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 8. VISUALIZATION\n# Explanation:\n# Plot confusion matrices and feature importances to understand model behavior.\n\n# Confusion Matrix Heatmap\ndef plot_confusion(y_true, y_pred, title):\n    cm = confusion_matrix(y_true, y_pred)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y), yticklabels=np.unique(y))\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title(title)\n    plt.show()\n\nplot_confusion(y_test, y_pred_log, \"Logistic Regression Confusion Matrix\")\nplot_confusion(y_test, y_pred_rf, \"Random Forest Confusion Matrix\")\n\n# Feature Importance (Random Forest)\nimportances = rf.feature_importances_\nfeat_names = X.columns\nfeat_imp_df = pd.DataFrame({'Feature': feat_names, 'Importance': importances}).sort_values(by='Importance', ascending=False)\n\nplt.figure(figsize=(10,6))\nsns.barplot(x='Importance', y='Feature', data=feat_imp_df.head(15))\nplt.title(\"Top 15 Feature Importances - Random Forest\")\nplt.show()","metadata":{"_uuid":"1bbe4f83-e30c-44ff-8736-e791d4c1941b","_cell_guid":"c05fcd2d-f351-4bc2-8ebb-1f648c592d53","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Discussion\n\nRandom Forest handles complex, non-linear patterns in EEG data, which likely explains the higher accuracy.\n\nLogistic Regression is simpler and linear, so it may not capture subtle EEG patterns as effectively.\n\nEEG feature importances can guide future neuroscience studies to focus on the most informative channels.\n\n### Limitations:\n\n* Only one dataset used\n* Model interpretability can be improved with SHAP or permutation importance\n\n### Future directions:\n\n* Test more models (SVM, XGBoost)\n* Explore deep learning on EEG time-series data","metadata":{"_uuid":"f98784c5-94bd-4076-b291-c63075724904","_cell_guid":"48e4041b-3251-46bc-8bdb-e39ec74f3ff6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}
